{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob as gb\n",
    "import os\n",
    "from fuzzywuzzy import fuzz\n",
    "import wikipedia\n",
    "from wikidata.client import Client\n",
    "from tqdm import tqdm\n",
    "client = Client()\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from fuzzywuzzy import process\n",
    "import itertools\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_soup(wd_key):\n",
    "    r = requests.get(f\"https://www.wikidata.org/w/api.php?action=wbgetentities&ids={wd_key}&format=xml\")\n",
    "    s = bs(r.content)\n",
    "    return s\n",
    "\n",
    "def find_party_claim(s):\n",
    "    try:\n",
    "        r = s.find_all(\"property\",{\"id\":\"P102\"})\n",
    "        r = [x for x in r if x.findChildren()[0].name == \"claim\"][0]\n",
    "        r = r('claim')[0]('mainsnak')[0]('datavalue')[0]('value')[0].attrs['id'] \n",
    "        return r\n",
    "    except Exception as e:\n",
    "        return \"na\"\n",
    "\n",
    "def find_parl_group(s,filename_df):\n",
    "    try:\n",
    "        positions = s.find(\"property\",{\"id\":\"P39\"}).find_all('claim')\n",
    "        positions = {p.find('value').attrs['id']:p.find(\"property\",{\"id\":\"P4100\"}).find('value').attrs['id'] for p in positions}\n",
    "        hit = positions[os.path.split(filename_df)[-1].split('-')[-1][:-4]]\n",
    "        return r\n",
    "    except Exception as e:\n",
    "        return \"na\"\n",
    "\n",
    "def find_party_wikipedia_election(df_elections,name):\n",
    "    candidates = set(df_elections['name'])\n",
    "    t = df_elections[df_elections['name'] == name].reset_index(drop=True)\n",
    "    if len(t) != 0:\n",
    "        return t['party'][0]\n",
    "\n",
    "    elif len(t) == 0:\n",
    "        name_fuzzy = process.extract(name, set(df_elections['name']), limit=1,scorer=fuzz.token_set_ratio)[0]\n",
    "        if name_fuzzy[1] > 90:\n",
    "            t = df_elections[df_elections['name'] == name_fuzzy[0]].reset_index(drop=True)\n",
    "            return t['party'][0]\n",
    "        else:\n",
    "            return \"na\"\n",
    "    else:\n",
    "        return \"na\"\n",
    "\n",
    "def find_party_wikipedia_byelection(df_byelections,name):\n",
    "    candidates = set(df_byelections['Winner'])\n",
    "    t = df_byelections[df_byelections['Winner'] == name].reset_index(drop=True)\n",
    "    if len(t) != 0:\n",
    "        return t['PartyNew'][0]\n",
    "\n",
    "    elif len(t) == 0:\n",
    "        name_fuzzy = process.extract(name, set(df_byelections['Winner']), limit=1,scorer=fuzz.token_set_ratio)[0]\n",
    "        if name_fuzzy[1] > 90:\n",
    "            t = df_byelections[df_byelections['Winner'] == name_fuzzy[0]].reset_index(drop=True)\n",
    "            return t['PartyNew'][0]\n",
    "        else:\n",
    "            return \"na\"\n",
    "    else:\n",
    "        return \"na\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_wikidata = [x for x in gb('/media/ruben/Elements/PhD/data/hansard/resources/wikidata-members/reformatted/*') if \"-Q\" in x and int(x.split('-Q')[0][-4:]) <= 1931 and int(x.split('-Q')[0][-4:]) >= 1917]\n",
    "list_elections = {x.split('-Q')[0].split('/')[-1]:f\"/media/ruben/Elements/PhD/data/hansard/resources/wikipedia-election-results/election-{x.split('-Q')[0].split('/')[-1]}.tsv\" for x in list_wikidata}\n",
    "df_byelections = pd.read_csv('/media/ruben/Elements/PhD/data/hansard/resources/wikipedia-election-results/byelection-1918-1931.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "801it [03:05,  4.32it/s]\n",
      "100%|██████████| 333/333 [02:02<00:00,  2.73it/s]\n",
      "631it [00:46, 13.47it/s]\n",
      "100%|██████████| 98/98 [00:36<00:00,  2.66it/s]\n",
      "625it [00:26, 23.31it/s]\n",
      "100%|██████████| 55/55 [00:21<00:00,  2.52it/s]\n",
      "676it [00:23, 28.53it/s]\n",
      "100%|██████████| 47/47 [00:14<00:00,  3.18it/s]\n",
      "653it [00:17, 38.29it/s]\n",
      "100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n",
      "677it [00:10, 66.41it/s]\n",
      "100%|██████████| 25/25 [00:08<00:00,  2.91it/s]\n"
     ]
    }
   ],
   "source": [
    "for wikidata_df in list_wikidata:\n",
    "    df = pd.read_csv(wikidata_df,sep='\\t')\n",
    "    df_elections = list_elections[wikidata_df.split('-Q')[0].split('/')[-1]]\n",
    "    df_elections = pd.read_csv(df_elections,sep='\\t')\n",
    "    \n",
    "    d = []\n",
    "\n",
    "    for c,i in tqdm(enumerate(df['mp_ref'])):\n",
    "\n",
    "        # Check if party = present, else continue\n",
    "        if str(df['party'][c]) == \"nan\":\n",
    "            filename_df = wikidata_df\n",
    "            wd_key = i.split('/')[-1][:-1]\n",
    "            soup = load_soup(wd_key)\n",
    "\n",
    "            party = find_party_claim(soup)\n",
    "\n",
    "            if party == \"na\":\n",
    "                party = find_parl_group(soup,filename_df)\n",
    "            if party == \"na\":\n",
    "                party = find_party_wikipedia_election(df_elections,df['mp'][c])\n",
    "            if party == \"na\":\n",
    "                party = find_party_wikipedia_byelection(df_byelections,df['mp'][c])\n",
    "            d.append([i,df['mp'][c],party])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    dfr = pd.DataFrame(d,columns=[\"wd\",\"name\",\"party\"])\n",
    "    dfr['party'] = [str(client.get(str(x)).label) if \"Q\" in x else x for x in tqdm(dfr['party'])] \n",
    "    dfr = {i:{\"name\":dfr['name'][c],\"party\":dfr['party'][c]}for c,i in enumerate(dfr['wd'])}\n",
    "    df['party'] = [x if str(x) != \"nan\" else dfr[df['mp_ref'][c]]['party'] for c,x in enumerate(df['party'])]\n",
    "\n",
    "    # Add party reference\n",
    "    pl = list(zip(df['party'],df['party_ref']))\n",
    "    pl = [[x[0],x[1]] for x in pl if str(x[1]) != \"nan\"]\n",
    "    party_dict = {}\n",
    "    for p in pl:\n",
    "        if p[0] not in party_dict.keys():\n",
    "            party_dict.update({p[0]:p[1]})\n",
    "        elif p[1] not in party_dict[p[0]]:\n",
    "            party_dict[p[0]].append(p[1])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    df['party_ref'] = [party_dict[x] if x in party_dict.keys() else \"nan\" for x in df['party']]\n",
    "    df.to_csv(wikidata_df[:-4] + \"-enriched.tsv\",index=False,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/media/ruben/Elements/PhD/data/hansard/resources/wikidata-members/reformatted/1918-Q41582582-enriched.tsv'"
      ]
     },
     "metadata": {},
     "execution_count": 99
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(wikidata_df,sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "677"
      ]
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}